{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ff7345-cd17-4bdc-b2ef-278ebafea4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goktug/anaconda3/envs/ceres/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "\n",
    "# This is a custom dataset class.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_dataset, is_train=True,\n",
    "                 transform=False, max_length=32000, sr=4000):\n",
    "        self.samples = []\n",
    "        self.labels_map = {}\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.sr = sr\n",
    "        self.read(path_dataset)\n",
    "\n",
    "    def read(self, path_dataset):\n",
    "        for idx_class, class_name in enumerate(os.listdir(path_dataset)):\n",
    "            path_class = os.path.join(path_dataset, class_name)\n",
    "            self.labels_map[class_name] = idx_class\n",
    "            if self.is_train:\n",
    "                path_class = os.path.join(path_class, \"train\")\n",
    "            else:\n",
    "                path_class = os.path.join(path_class, \"test\")\n",
    "\n",
    "            for idx, file_name in enumerate(os.listdir(path_class)):\n",
    "                path_file = os.path.join(path_class, file_name)\n",
    "                waveform, sr = torchaudio.load(path_file)\n",
    "                metadata = torchaudio.info(path_file)                \n",
    "                if self.transform:\n",
    "                    transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sr)\n",
    "                    waveform = transform(waveform)\n",
    "                    waveform = self.padding(waveform, self.max_length)\n",
    "                    print(waveform.shape)\n",
    "                    sr = self.sr\n",
    "\n",
    "                self.samples.append((waveform, idx_class, sr))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index][0], self.samples[index][1], self.samples[index][2]\n",
    "\n",
    "    def padding(self, waveform, max_len):\n",
    "        # Pad the waveform\n",
    "        length_waveform = waveform.shape[1]\n",
    "        if length_waveform < max_len:\n",
    "            waveform = torch.cat((waveform, torch.zeros((1, max_len - length_waveform))), dim=1)\n",
    "        return waveform\n",
    "\n",
    "    def getLabelsMap(self):\n",
    "        return self.labels_map\n",
    "\n",
    "    def getLabelCount(self):\n",
    "        return len(self.labels_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e5d6d9-7af1-492b-a861-392521df13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38508992-d996-43f9-83fc-e2316752857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_dataset\n",
    "from model import M5\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sounddevice as sd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def plot_waveform(waveform, sample_rate):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c + 1}\")\n",
    "    figure.suptitle(\"waveform\")\n",
    "    plt.show(block=True)\n",
    "\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tensors, targets = [], []\n",
    "    for waveform, idx_class, sr in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [torch.tensor(idx_class)]\n",
    "\n",
    "        # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "\n",
    "def number_of_correct(pred, target):\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "def padding(waveform, max_len):\n",
    "    # Pad the waveform\n",
    "    length_waveform = waveform.shape[1]\n",
    "    if length_waveform < max_len:\n",
    "        waveform = torch.cat((waveform, torch.zeros((1, max_len - length_waveform))), dim=1)\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    return tensor.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9bbd9e-4184-4819-8b77-e06816940116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "path_dataset = \"/home/goktug/projects/Ceres/dataset/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: \", device)\n",
    "\n",
    "sr_target = 8000\n",
    "max_length = 32000\n",
    "batch_size = 2\n",
    "transform = True\n",
    "\n",
    "custom_dataset_train = custom_dataset.CustomDataset(path_dataset, True, transform,\n",
    "                                                    max_length, sr_target)\n",
    "train_loader = torch.utils.data.DataLoader(custom_dataset_train, batch_size=batch_size,\n",
    "                                           shuffle=True, num_workers=1, collate_fn=collate_fn,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "custom_dataset_test = custom_dataset.CustomDataset(path_dataset, False, transform, max_length, sr_target)\n",
    "test_loader = torch.utils.data.DataLoader(custom_dataset_test, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=1, collate_fn=collate_fn,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "575a8b65-07c8-403a-89b0-71fbb36d67b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train dataset:  3066\n",
      "Number of samples in test dataset:  771\n",
      "Sample rate:  8000\n",
      "Wavelength:  16000\n",
      "{'kapa': 0, 'ac': 1}\n",
      "{'kapa': 0, 'ac': 1}\n"
     ]
    }
   ],
   "source": [
    "# Iterate over batches train:\n",
    "sample_waveform = None\n",
    "for idx, (waveforms, targets) in enumerate(train_loader):\n",
    "    # print(\"Batch index: \", idx)\n",
    "    # print(\"Waveforms shape: \", len(waveforms))\n",
    "    # print(\"Targets: \", targets)\n",
    "    sample_waveform = waveforms[0]\n",
    "    break\n",
    "    # plot_waveform(waveforms[0], sr_target)\n",
    "\n",
    "print(\"Number of samples in train dataset: \", len(custom_dataset_train))\n",
    "print(\"Number of samples in test dataset: \", len(custom_dataset_test))\n",
    "print(\"Sample rate: \", sr_target)\n",
    "print(\"Wavelength: \", len(sample_waveform[0]))\n",
    "print(custom_dataset_train.labels_map)\n",
    "print(custom_dataset_test.labels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b73edf-a9a9-4e97-8bc4-9bfdc3b1b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M5(n_input=sample_waveform.shape[0],\n",
    "           n_output=custom_dataset_train.getLabelCount())\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb55eb4-618c-4e68-9260-96327ab82b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  0  Loss:  0.21246020459727974\n",
      "Test accuracy:  0.9559014267185474\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#print(\"Batch: \", batch_idx, \" Sample count: \", sample_count, \" Loss: \", loss.item())\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/ceres/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:67\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     66\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ceres/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ceres/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    107\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/ceres/lib/python3.8/site-packages/torch/optim/functional.py:83\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     80\u001b[0m bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     86\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "losses_train_list = []\n",
    "for epoch in range(12):\n",
    "\n",
    "    loss_epoch = 0\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for batch_idx, (waveforms, targets) in enumerate(train_loader):\n",
    "        sample_count = len(waveforms)\n",
    "        waveforms = waveforms.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output_ac = model(waveforms)\n",
    "        loss = F.nll_loss(output_ac.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"Batch: \", batch_idx, \" Sample count: \", sample_count, \" Loss: \", loss.item())\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    loss_epoch /= len(train_loader)\n",
    "    losses_train_list.append(loss_epoch)\n",
    "    print(\"\\nEpoch: \", epoch, \" Loss: \", loss_epoch)\n",
    "    \n",
    "    \n",
    "    # Test\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (waveforms, targets) in enumerate(test_loader):\n",
    "            waveforms = waveforms.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output_ac = model(waveforms)\n",
    "            pred = get_likely_index(output_ac)\n",
    "            correct += number_of_correct(pred, targets)\n",
    "\n",
    "    print(\"Test accuracy: \", correct / len(custom_dataset_test))\n",
    "    print(\"-----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c708c9-ca6d-4946-afb6-58a5eaf17aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ac = \"/home/goktug/Downloads/ac.wav\"\n",
    "path_kapa = \"/home/goktug/Downloads/kapa.wav\"\n",
    "waveform_ac, sr_ac = torchaudio.load(path_ac)\n",
    "waveform_ac = waveform_ac[0]\n",
    "print(\"Loaded audio with sample rate: \", sr_ac)\n",
    "\n",
    "waveform_ac = waveform_ac.reshape(1, -1)\n",
    "transform = torchaudio.transforms.Resample(orig_freq=sr_ac, new_freq=sr_target)\n",
    "waveform_ac = transform(waveform_ac)\n",
    "waveform_ac = padding(waveform_ac, max_length)\n",
    "waveform_ac.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
